# ECG Adversarial Robustness

> **Adversarial attacks on Deep Learning models for ECG classification**  
> Studio delle vulnerabilitÃ  di modelli di deep learning per la classificazione automatica di tracciati ECG, in contesto centralizzato e federated learning, con focus su attacchi *test-time* e *train-time*.

---

## ğŸ“‹ Descrizione del Progetto

L'obiettivo del progetto Ã¨ analizzare e valutare la **robustezza** di modelli di classificazione ECG in scenari avversi.  
Il lavoro copre:

- **Attacchi evasion** (*test-time*) â†’ perturbazioni aggiunte agli input per ingannare un modello giÃ  addestrato (es. FGSM, PGD, AdvGAN).
- **Attacchi poisoning** (*train-time*) â†’ inserimento di dati falsi plausibili per compromettere l'addestramento (es. VagueGAN).
- **Scenario centralizzato e Federated Learning** â†’ valutazione della sicurezza in contesti distribuiti.
- **Strategie di difesa** â†’ metodi di robustezza a livello di training, preprocessing e detection.

---

## ğŸ— Struttura del Repository

```
ecg-adv-robustness/
â”œâ”€â”€ README.md
â”œâ”€â”€ environment.yaml
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # dati ECG originali (immagini)
â”‚   â”œâ”€â”€ processed/              # dati preprocessati per il training
â”‚   â”œâ”€â”€ preprocess_dataset.py   # funzioni di preprocessing e split
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ ecg_classifier_cnn.py    # modello baseline (CNN/ResNet)
â”‚   â”œâ”€â”€ utils.py                # utilitÃ  per i modelli
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train_baseline.py       # training su dati puliti
â”‚   â”œâ”€â”€ eval_baseline.py        # valutazione modelli
â”‚
â”œâ”€â”€ attacks/
â”‚   â”œâ”€â”€ gradient_based/         # attacchi basati su gradienti
â”‚   â”‚     â”œâ”€â”€ fgsm.py
â”‚   â”‚     â”œâ”€â”€ pgd.py
â”‚   â”‚     â”œâ”€â”€ cw.py
â”‚   â”‚     â”œâ”€â”€ deepfool.py
â”‚   â”‚
â”‚   â”œâ”€â”€ gan_based/
â”‚   â”‚     â”œâ”€â”€ advGAN/           # attacchi evasion con GAN
â”‚   â”‚     â”‚     â”œâ”€â”€ generator_2d.py
â”‚   â”‚     â”‚     â”œâ”€â”€ discriminator_2d.py
â”‚   â”‚     â”‚     â”œâ”€â”€ advgan.py
â”‚   â”‚     â”‚     â”œâ”€â”€ train_advgan.py
â”‚   â”‚     â”‚     â”œâ”€â”€ attack_advgan.py
â”‚   â”‚     â”‚
â”‚   â”‚     â”œâ”€â”€ poisoningGAN/    # attacchi poisoning con GAN
â”‚   â”‚           â”œâ”€â”€ generator_2d.py
â”‚   â”‚           â”œâ”€â”€ discriminator_2d.py
â”‚   â”‚           â”œâ”€â”€ poisoning_gan.py
â”‚   â”‚           â”œâ”€â”€ train_poisoning_gan.py
â”‚   â”‚           â”œâ”€â”€ inject_poisoning.py
â”‚
â”œâ”€â”€ defenses/
â”‚   â”œâ”€â”€ training_based/
â”‚   â”‚     â”œâ”€â”€ adversarial_training.py
â”‚   â”‚     â”œâ”€â”€ def_distillation.py
â”‚   â”‚
â”‚   â”œâ”€â”€ preprocessing_based/
â”‚   â”‚     â”œâ”€â”€ filtering.py
â”‚   â”‚     â”œâ”€â”€ denoising.py
â”‚   â”‚
â”‚   â”œâ”€â”€ detection_based/
â”‚   â”‚     â”œâ”€â”€ anomaly.py
â”‚
â”œâ”€â”€ federated/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚     â”œâ”€â”€ fl_server.py
â”‚   â”‚     â”œâ”€â”€ fl_client.py
â”‚   â”‚
â”‚   â”œâ”€â”€ fl_attack_wrapper.py
â”‚   â”‚
â”‚   â”œâ”€â”€ fl_defense_wrapper.py
â”‚   â”‚
â”‚   â”œâ”€â”€ run_federated.py
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ notebooks/
â”‚
â””â”€â”€ utils/
    â”œâ”€â”€ metrics.py
    â”œâ”€â”€ viz.py
    â””â”€â”€ seed.py
```

---

## ğŸ“Š Dataset

Il progetto supporta dataset ECG in formato **immagine**.  
Esempi utilizzabili:
- [PTB-XL](https://physionet.org/content/ptb-xl/1.0.1/)
- MIT-BIH arrhythmia dataset (convertito in immagini)

Il preprocessing (`prepreprocess_dataset.py`) include:
- Ridimensionamento e normalizzazione delle immagini
- Conversione a tensori PyTorch
- Creazione di split train/val/test

---

## ğŸ§  Modelli

### `ecg_classifier_cnn.py`
- Implementazione di una CNN 2D semplice o una ResNet adattata per immagini ECG.
- Configurabile tramite file di configurazione in `experiments/configs/`.

---

## âš”ï¸ Attacchi Implementati

### Gradient-based (*Evasion*)
- **FGSM** â€“ Fast Gradient Sign Method
- **PGD** â€“ Projected Gradient Descent
- **CW** â€“ Carlini & Wagner
- **DeepFool**

### GAN-based
- **AdvGAN** (*Evasion*) â€“ Generatore di perturbazioni impercettibili per immagini ECG.
- **VagueGAN** (*Poisoning*) â€“ Generatore di dati plausibili ma falsi per corrompere il dataset di training.

---

## ğŸ›¡ Difese Implementate

### Training-based
- **Adversarial Training** â€“ retraining del modello con campioni avversari
- **Defensive Distillation** â€“ riduce la sensibilitÃ  ai gradienti

### Preprocessing-based
- **Filtering** â€“ filtri passa-basso, smoothing
- **Denoising** â€“ autoencoder di denoising, trasformate wavelet

### Detection-based
- **Anomaly Score** â€“ calcolo di score statistici per rilevare input sospetti

---

## ğŸ¤ Federated Learning

La cartella `federated/` contiene:
- **Server FL** â€“ aggregazione dei modelli
- **Client FL** â€“ addestramento locale
- **Wrapper di attacco/difesa** â€“ adattamento di attacchi e difese al contesto federato
- **Simulazioni** â€“ avvio di esperimenti FL con client benigni e malevoli

---

## ğŸš€ Come iniziare

### 1. Creare l'ambiente
```bash
conda env create -f environment.yaml
conda activate ecg-adv-robustness
```

### 2. Preparare i dati
```bash
python data/preprocess_dataset.py --input data/raw --output data/processed
```

### 3. Addestrare modello baseline
```bash
python training/train_baseline.py --config experiments/configs/baseline.yaml
```

### 4. Lanciare un attacco FGSM
```bash
python attacks/gradient_based/fgsm.py --model models/ecg_classifier_cnn.py --data data/processed
```

### 5. Addestrare AdvGAN
```bash
python attacks/gan_based/advgan/train_advgan.py --config experiments/configs/advgan.yaml
```

### 6. Simulare FL con attacco Poisoning
```bash
python federated/run_federated.py --config experiments/configs/fl_poisoning.yaml
```

---

## ğŸ“ˆ Metriche

Le metriche principali calcolate in `utils/metrics.py`:
- Accuratezza
- Precisione / Recall / F1-score
- Robust Accuracy (su campioni avversari)
- Tasso di rilevamento anomalie (per difese detection-based)

---

## ğŸ“Œ Note
- Questo progetto Ã¨ a scopo **accademico**.  
- Gli attacchi avversari qui implementati devono essere usati solo in contesti controllati e per ricerca.

---
